{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T09:44:49.677988Z",
     "start_time": "2026-01-05T09:44:48.403420Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.signal import butter, filtfilt\n",
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import spsolve\n",
    "\n",
    "try:\n",
    "    from sklearn.linear_model import Lasso\n",
    "except Exception:\n",
    "    Lasso = None\n",
    "from pybaselines import Baseline\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 4)\n",
    "plt.rcParams[\"axes.grid\"] = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3711cb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = r\"your/path/here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16606b8d9a8214c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T09:44:49.700011Z",
     "start_time": "2026-01-05T09:44:49.682992Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.signal import butter, filtfilt, decimate\n",
    "def list_doric_channels(path):\n",
    "    with h5py.File(path, \"r\") as f:\n",
    "        base = f[\"DataAcquisition\"][\"FPConsole\"][\"Signals\"][\"Series0001\"]\n",
    "        chans = []\n",
    "        if \"LockInAOUT02\" in base:\n",
    "            for k in base[\"LockInAOUT02\"].keys():\n",
    "                if k.startswith(\"AIN\"):\n",
    "                    chans.append(k)\n",
    "        chans = sorted(chans)\n",
    "\n",
    "        digital = []\n",
    "        if \"DigitalIO\" in base:\n",
    "            for k in base[\"DigitalIO\"].keys():\n",
    "                if k.startswith(\"DIO\"):\n",
    "                    digital.append(k)\n",
    "        return chans, digital\n",
    "\n",
    "def load_doric(path, channel=\"AIN01\", signal_folder=\"LockInAOUT02\", ref_folder=\"LockInAOUT01\",\n",
    "              trigger_name=None):\n",
    "    \"\"\"\n",
    "    Returns dict with:\n",
    "      time, sig465, ref405, fs, (optional) trig_time, trig\n",
    "    \"\"\"\n",
    "    with h5py.File(path, \"r\") as f:\n",
    "        base = f[\"DataAcquisition\"][\"FPConsole\"][\"Signals\"][\"Series0001\"]\n",
    "\n",
    "        sig = np.asarray(base[signal_folder][channel][()], float)\n",
    "        ref = np.asarray(base[ref_folder][channel][()], float)\n",
    "\n",
    "        # time: prefer the matching folder time if size matches\n",
    "        t_sig = np.asarray(base[signal_folder][\"Time\"][()], float) if \"Time\" in base[signal_folder] else np.array([])\n",
    "        t_ref = np.asarray(base[ref_folder][\"Time\"][()], float) if \"Time\" in base[ref_folder] else np.array([])\n",
    "\n",
    "        if t_sig.size == sig.size:\n",
    "            t = t_sig\n",
    "        elif t_ref.size == sig.size:\n",
    "            t = t_ref\n",
    "        else:\n",
    "            # fallback\n",
    "            dt = np.nanmedian(np.diff(t_sig)) if t_sig.size > 2 else 1/1000\n",
    "            t = np.arange(sig.size) * dt\n",
    "\n",
    "        # if ref length differs, interpolate onto t if possible\n",
    "        if ref.size != sig.size:\n",
    "            if t_ref.size == ref.size:\n",
    "                ref = np.interp(t, t_ref, ref)\n",
    "            else:\n",
    "                ref = np.resize(ref, sig.size)\n",
    "\n",
    "        # sampling rate\n",
    "        fs = 1.0 / float(np.nanmedian(np.diff(t))) if t.size > 2 else np.nan\n",
    "\n",
    "        # optional digital trigger overlay\n",
    "        trig_time = None\n",
    "        trig = None\n",
    "        if trigger_name:\n",
    "            if \"DigitalIO\" in base and trigger_name in base[\"DigitalIO\"]:\n",
    "                dio = base[\"DigitalIO\"]\n",
    "                trig = np.asarray(dio[trigger_name][()], float)\n",
    "                trig_time = np.asarray(dio[\"Time\"][()], float) if \"Time\" in dio else None\n",
    "\n",
    "                # if lengths mismatch, interpolate signals to trigger time (like your Doric logic)\n",
    "                if trig_time is not None and trig_time.size and trig_time.size != t.size:\n",
    "                    sig = np.interp(trig_time, t, sig)\n",
    "                    ref = np.interp(trig_time, t, ref)\n",
    "                    t = trig_time\n",
    "                    fs = 1.0 / float(np.nanmedian(np.diff(t))) if t.size > 2 else fs\n",
    "\n",
    "    out = {\"time\": t, \"sig465\": sig, \"ref405\": ref, \"fs\": fs}\n",
    "    if trig is not None and trig_time is not None:\n",
    "        out[\"trig_time\"] = trig_time\n",
    "        out[\"trig\"] = trig\n",
    "    return out\n",
    "import numpy as np\n",
    "\n",
    "def ols_fit(x, y):\n",
    "    x = np.asarray(x, float)\n",
    "    y = np.asarray(y, float)\n",
    "    m = np.isfinite(x) & np.isfinite(y)\n",
    "    if np.sum(m) < 10:\n",
    "        return 1.0, 0.0\n",
    "    X = np.vstack([x[m], np.ones(np.sum(m))]).T\n",
    "    coef, *_ = np.linalg.lstsq(X, y[m], rcond=None)\n",
    "    return float(coef[0]), float(coef[1])\n",
    "def preprocess_signal(sig, fs_raw, target_fs=100, lpf_cutoff=3):\n",
    "    \"\"\"\n",
    "    1\\) Low-pass filter (Butterworth, lpf_cutoff Hz)\n",
    "    2\\) Decimate to target sampling rate (target_fs Hz)\n",
    "    \"\"\"\n",
    "    nyquist = 0.5 * fs_raw\n",
    "    normal_cutoff = lpf_cutoff / nyquist\n",
    "    b, a = butter(N=2, Wn=normal_cutoff, btype=\"low\", analog=False)\n",
    "    sig_filtered = filtfilt(b, a, sig)\n",
    "\n",
    "    q = int(fs_raw / target_fs)\n",
    "    if q > 1:\n",
    "        sig_downsampled = decimate(sig_filtered, q)\n",
    "        real_fs = fs_raw / q\n",
    "    else:\n",
    "        sig_downsampled = sig_filtered\n",
    "        real_fs = fs_raw\n",
    "\n",
    "    return sig_downsampled, real_fs\n",
    "\n",
    "\n",
    "def compute_arpls_baselines(\n",
    "    baseline_fitter,\n",
    "    sig_f,\n",
    "    ref_f,\n",
    "    lam=1e9,\n",
    "    diff_order=2,\n",
    "    max_iter=50,\n",
    "    tol=1e-3,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute arPLS baselines for signal and reference.\n",
    "\n",
    "    Parameters:\n",
    "        baseline_fitter: pybaselines.Baseline instance, already set up with x_data.\n",
    "        sig_f: 1D array-like, preprocessed signal channel.\n",
    "        ref_f: 1D array-like, preprocessed reference channel.\n",
    "        lam: smoothing parameter.\n",
    "        diff_order: difference order for the penalty.\n",
    "        max_iter: maximum iterations for arPLS.\n",
    "        tol: convergence tolerance.\n",
    "\n",
    "    Returns:\n",
    "        b_sig_arpls, b_ref_arpls\n",
    "    \"\"\"\n",
    "    b_sig_arpls, _ = baseline_fitter.arpls(\n",
    "        sig_f, lam=lam, diff_order=diff_order,\n",
    "        max_iter=max_iter, tol=tol\n",
    "    )\n",
    "    b_ref_arpls, _ = baseline_fitter.arpls(\n",
    "        ref_f, lam=lam, diff_order=diff_order,\n",
    "        max_iter=max_iter, tol=tol\n",
    "    )\n",
    "    return b_sig_arpls, b_ref_arpls\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_motion_corrected_dff(sig_f, ref_f, b_sig, b_ref):\n",
    "    \"\"\"\n",
    "    Computes motion-corrected dF/F using the 'Standardized' method:\n",
    "    1) Calculate dF/F for both signal and reference channels separately.\n",
    "       dff_sig_raw = (sig - b_sig) / b_sig\n",
    "       dff_ref_raw = (ref - b_ref) / b_ref\n",
    "    2) Fit dff_ref_raw to dff_sig_raw using OLS (y = ax + b).\n",
    "    3) Subtract the fitted reference from the signal dF/F.\n",
    "       dff_mc = dff_sig_raw - (a * dff_ref_raw + b)\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 1. Calculate raw dF/F for each channel ---\n",
    "    # Prepare denominators (avoid division by zero)\n",
    "    den_sig = np.asarray(b_sig, float).copy()\n",
    "    den_sig[np.abs(den_sig) < 1e-12] = np.nan\n",
    "\n",
    "    den_ref = np.asarray(b_ref, float).copy()\n",
    "    den_ref[np.abs(den_ref) < 1e-12] = np.nan\n",
    "\n",
    "    # Calculate standard dF/F (percent change) for each\n",
    "    dff_sig_raw = (sig_f - b_sig) / den_sig\n",
    "    dff_ref_raw = (ref_f - b_ref) / den_ref\n",
    "\n",
    "    # --- 2. Fit Reference dF/F to Signal dF/F ---\n",
    "    # We fit: dff_sig_raw ~ a * dff_ref_raw + b\n",
    "    a, b = ols_fit(dff_ref_raw, dff_sig_raw)\n",
    "\n",
    "    # --- 3. Subtract to get Motion-Corrected dF/F ---\n",
    "    # The fitted reference represents the motion/artifact component in dF/F space\n",
    "    fitted_ref = (a * dff_ref_raw + b)\n",
    "    dff_mc = dff_sig_raw - fitted_ref\n",
    "\n",
    "    # Return dictionary with keys compatible with your analysis pipeline\n",
    "    # Note: 'sig_det' and 'ref_det' now refer to the raw dF/F traces\n",
    "    return {\n",
    "        \"sig_det\": dff_sig_raw,   # Now holds raw dF/F of signal\n",
    "        \"ref_det\": dff_ref_raw,   # Now holds raw dF/F of reference\n",
    "        \"a\": a,                   # Slope of the regression in dF/F space\n",
    "        \"b\": b,                   # Intercept of the regression\n",
    "        \"delta_mc\": dff_mc,       # The final motion-corrected dF/F\n",
    "        \"dff\": dff_mc,            # Same as delta_mc (kept for compatibility)\n",
    "    }\n",
    "def list_doric_files(folder_path):\n",
    "    \"\"\"\n",
    "    Return a list of full paths to all .doric files in the given folder.\n",
    "    \"\"\"\n",
    "    doric_files = []\n",
    "    for fname in os.listdir(folder_path):\n",
    "        if fname.lower().endswith(\".doric\"):\n",
    "            doric_files.append(os.path.join(folder_path, fname))\n",
    "    return doric_files\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def qc_sig_ref_correlation(sig, ref, title=\"\", max_points=8000, ax=None):\n",
    "    \"\"\"\n",
    "    Scatter sig vs ref with OLS fit + Pearson R, R^2.\n",
    "    Downsamples points for plotting but computes stats on full data.\n",
    "    Returns dict with r, r2, slope, intercept, n.\n",
    "    \"\"\"\n",
    "    sig = np.asarray(sig, float)\n",
    "    ref = np.asarray(ref, float)\n",
    "\n",
    "    m = np.isfinite(sig) & np.isfinite(ref)\n",
    "    sig = sig[m]\n",
    "    ref = ref[m]\n",
    "    n = int(sig.size)\n",
    "\n",
    "    out = {\"r\": np.nan, \"r2\": np.nan, \"slope\": np.nan, \"intercept\": np.nan, \"n\": n}\n",
    "\n",
    "    if n < 10:\n",
    "        return out\n",
    "\n",
    "    # Pearson R\n",
    "    s0 = sig - np.mean(sig)\n",
    "    r0 = ref - np.mean(ref)\n",
    "    denom = np.sqrt(np.sum(s0**2) * np.sum(r0**2))\n",
    "    if denom > 0:\n",
    "        r = float(np.sum(s0 * r0) / denom)\n",
    "    else:\n",
    "        r = np.nan\n",
    "    out[\"r\"] = r\n",
    "    out[\"r2\"] = float(r**2) if np.isfinite(r) else np.nan\n",
    "\n",
    "    # OLS fit sig = a*ref + b\n",
    "    X = np.vstack([ref, np.ones_like(ref)]).T\n",
    "    coef, *_ = np.linalg.lstsq(X, sig, rcond=None)\n",
    "    a, b = float(coef[0]), float(coef[1])\n",
    "    out[\"slope\"] = a\n",
    "    out[\"intercept\"] = b\n",
    "\n",
    "    # Plot\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(4.2, 3.6))\n",
    "\n",
    "    # downsample points for display\n",
    "    if n > max_points:\n",
    "        idx = np.random.default_rng(0).choice(n, size=max_points, replace=False)\n",
    "        xs = ref[idx]\n",
    "        ys = sig[idx]\n",
    "    else:\n",
    "        xs, ys = ref, sig\n",
    "\n",
    "    ax.scatter(xs, ys, s=6, alpha=0.25, edgecolors=\"none\")\n",
    "\n",
    "    # fit line over displayed x-range\n",
    "    xlo, xhi = np.nanpercentile(ref, [1, 99])\n",
    "    xx = np.linspace(xlo, xhi, 200)\n",
    "    yy = a * xx + b\n",
    "    ax.plot(xx, yy, lw=1.8)\n",
    "\n",
    "    ax.set_title(title, fontsize=10)\n",
    "    ax.set_xlabel(\"Reference 405 (a.u.)\")\n",
    "    ax.set_ylabel(\"Signal 465 (a.u.)\")\n",
    "\n",
    "    # cosmetics\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.grid(True, alpha=0.15)\n",
    "\n",
    "    ax.text(\n",
    "        0.02, 0.98,\n",
    "        f\"R = {out['r']:.3f}\\nRÂ² = {out['r2']:.3f}\\na = {a:.3g}\",\n",
    "        transform=ax.transAxes, va=\"top\", ha=\"left\", fontsize=9\n",
    "    )\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1831a43717d24a59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T09:44:49.710010Z",
     "start_time": "2026-01-05T09:44:49.704009Z"
    }
   },
   "outputs": [],
   "source": [
    "doric_paths = list_doric_files(folder)\n",
    "print(doric_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2eacc3b29f3440e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T09:44:52.248098Z",
     "start_time": "2026-01-05T09:44:49.716118Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results = {}\n",
    "qc_list = []\n",
    "\n",
    "# Parameters\n",
    "target_fs = 30\n",
    "lpf_cutoff = 3\n",
    "lam = 1e11\n",
    "diff_order = 2\n",
    "max_iter = 50\n",
    "tol = 1e-3\n",
    "\n",
    "plot_window = [300, 380]  # or None\n",
    "\n",
    "def apply_plot_window(t: np.ndarray, *series: np.ndarray, plot_window=None):\n",
    "    if plot_window is None:\n",
    "        mask = np.ones_like(t, dtype=bool)\n",
    "        return (t, *series, mask)\n",
    "\n",
    "    lo, hi = float(plot_window[0]), float(plot_window[1])\n",
    "    if lo > hi:\n",
    "        lo, hi = hi, lo\n",
    "\n",
    "    mask = (t >= lo) & (t <= hi)\n",
    "    t2 = t[mask]\n",
    "    series2 = [s[mask] if s is not None else None for s in series]\n",
    "    return (t2, *series2, mask)\n",
    "\n",
    "for path in doric_paths:\n",
    "    filename = os.path.basename(path)\n",
    "\n",
    "    if '-' in filename:\n",
    "        animal_id = filename.split('-')[0]\n",
    "    else:\n",
    "        animal_id = os.path.splitext(filename)[0]\n",
    "\n",
    "    print(f\"Processing: {filename} -> Animal ID: {animal_id}\")\n",
    "\n",
    "    rec = load_doric(path, trigger_name=\"DIO02\")\n",
    "\n",
    "    sig = rec[\"sig465\"]\n",
    "    ref = rec[\"ref405\"]\n",
    "    fs  = rec[\"fs\"]\n",
    "\n",
    "    dio_raw  = rec.get(\"trig\", None)\n",
    "    dio_time = rec.get(\"trig_time\", None)\n",
    "\n",
    "    # -------------------- PURE QC: sig vs ref correlation (raw) --------------------\n",
    "    fig_qc, ax_qc = plt.subplots(figsize=(4.2, 3.6))\n",
    "    qc = qc_sig_ref_correlation(sig, ref, title=f\"{animal_id} | raw sig vs ref\", ax=ax_qc)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # -------------------- Preprocess --------------------\n",
    "    sig_f, fs_sig = preprocess_signal(sig, fs, target_fs=target_fs, lpf_cutoff=lpf_cutoff)\n",
    "    ref_f, fs_ref = preprocess_signal(ref, fs, target_fs=target_fs, lpf_cutoff=lpf_cutoff)\n",
    "\n",
    "    dt_ds = 1.0 / fs_sig\n",
    "    t_ds = np.arange(sig_f.size) * dt_ds\n",
    "\n",
    "    baseline_fitter = Baseline(x_data=t_ds)\n",
    "    b_sig, b_ref = compute_arpls_baselines(\n",
    "        baseline_fitter, sig_f, ref_f,\n",
    "        lam=lam, diff_order=diff_order, max_iter=max_iter, tol=tol\n",
    "    )\n",
    "\n",
    "    dff_results = compute_motion_corrected_dff(sig_f, ref_f, b_sig, b_ref)\n",
    "    dff_mc = dff_results[\"dff\"]\n",
    "    dff_z = (dff_mc - np.nanmean(dff_mc)) / np.nanstd(dff_mc)\n",
    "\n",
    "    if dio_raw is not None and dio_time is not None:\n",
    "        dio_ds = np.interp(t_ds, dio_time, dio_raw)\n",
    "    else:\n",
    "        dio_ds = np.zeros_like(t_ds)\n",
    "\n",
    "    results[animal_id] = {\n",
    "        \"t\": t_ds,\n",
    "        \"dff\": dff_mc,\n",
    "        \"zscore\": dff_z,\n",
    "        \"dio2\": dio_ds,\n",
    "        \"qc_sig_ref\": qc,            # <-- stored here\n",
    "        \"qc_sig_std\": float(np.nanstd(sig)),\n",
    "        \"qc_ref_std\": float(np.nanstd(ref)),\n",
    "        \"file\": filename\n",
    "\n",
    "    }\n",
    "\n",
    "    qc_list.append({\n",
    "        \"animal\": animal_id,\n",
    "        \"file\": filename,\n",
    "        \"r\": qc.get(\"r\", np.nan),\n",
    "        \"r2\": qc.get(\"r2\", np.nan),\n",
    "        \"slope\": qc.get(\"slope\", np.nan),\n",
    "        \"intercept\": qc.get(\"intercept\", np.nan),\n",
    "        \"n\": qc.get(\"n\", 0),\n",
    "        \"sig_std\": float(np.nanstd(sig)),\n",
    "        \"ref_std\": float(np.nanstd(ref)),\n",
    "    })\n",
    "\n",
    "    # -------------------- Plot window --------------------\n",
    "    t_plot, dff_z_plot, dio_plot, _mask = apply_plot_window(\n",
    "        t_ds, dff_z, dio_ds, plot_window=plot_window\n",
    "    )\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 4))\n",
    "    ax1.set_xlabel(\"Time (s)\")\n",
    "    ax1.set_ylabel(\"Z-score\")\n",
    "    ax1.plot(t_plot, dff_z_plot, lw=1, label=\"dF/F Z-score\")\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel(\"DIO2\")\n",
    "    ax2.fill_between(t_plot, dio_plot, alpha=0.3, step=\"mid\", label=\"DIO2\", color='C3')\n",
    "    ax2.set_ylim(-0.1, 1.5)\n",
    "\n",
    "    if plot_window is not None:\n",
    "        plt.title(f\"Animal: {animal_id} | Window: {plot_window[0]}-{plot_window[1]} s\")\n",
    "    else:\n",
    "        plt.title(f\"Animal: {animal_id} | File: {filename}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c391089ddfdc2313",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T09:44:52.659396Z",
     "start_time": "2026-01-05T09:44:52.651671Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "def _compute_psth_matrix(\n",
    "    t: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    event_times: np.ndarray,\n",
    "    window: Tuple[float, float],\n",
    "    baseline_win: Tuple[float, float],\n",
    "    resample_hz: float,\n",
    "    smooth_sigma_s: float = 0.0,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      tvec (relative time), mat (n_events x n_samples) with NaNs if missing\n",
    "    \"\"\"\n",
    "    t = np.asarray(t, float)\n",
    "    y = np.asarray(y, float)\n",
    "    ev = np.asarray(event_times, float)\n",
    "    ev = ev[np.isfinite(ev)]\n",
    "    if ev.size == 0:\n",
    "        return np.array([], float), np.zeros((0, 0), float)\n",
    "\n",
    "    dt = 1.0 / float(resample_hz)\n",
    "    tvec = np.arange(window[0], window[1] + 0.5 * dt, dt)\n",
    "\n",
    "    mat = np.full((ev.size, tvec.size), np.nan, float)\n",
    "\n",
    "    for i, et in enumerate(ev):\n",
    "        # baseline\n",
    "        bmask = (t >= et + baseline_win[0]) & (t <= et + baseline_win[1])\n",
    "        base = y[bmask]\n",
    "        if base.size < 5 or not np.any(np.isfinite(base)):\n",
    "            continue\n",
    "        bmean = np.nanmean(base)\n",
    "        bstd = np.nanstd(base)\n",
    "        if not np.isfinite(bstd) or bstd <= 1e-12:\n",
    "            bstd = 1.0\n",
    "\n",
    "        # extract window and interpolate onto tvec\n",
    "        wmask = (t >= et + window[0]) & (t <= et + window[1])\n",
    "        tw = t[wmask] - et\n",
    "        yw = y[wmask]\n",
    "        good = np.isfinite(tw) & np.isfinite(yw)\n",
    "        if np.sum(good) < 5:\n",
    "            continue\n",
    "        # sparse interpolation\n",
    "        mat[i, :] = np.interp(tvec, tw[good], (yw[good] - bmean) / bstd)\n",
    "\n",
    "    if smooth_sigma_s and smooth_sigma_s > 0:\n",
    "        # simple gaussian smoothing along time axis\n",
    "        sigma = smooth_sigma_s * resample_hz\n",
    "        mat = gaussian_filter1d(mat, sigma=sigma, axis=1, mode=\"nearest\")\n",
    "\n",
    "    return tvec, mat\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2721e099b285e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T09:44:52.974108Z",
     "start_time": "2026-01-05T09:44:52.673005Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---- User Options ----\n",
    "PLOT_WINDOW = None          # [start_s, end_s] or None for full session\n",
    "DIO_THRESHOLD = 0.5         # Voltage threshold for digital IO\n",
    "MIN_EVENT_GAP_S = 0.2      # Debounce: ignore events closer than 20ms\n",
    "BOUT_GAP_S = \"auto\"         # New burst if IEI > this (seconds). \"auto\" uses log-KMeans.\n",
    "MIN_LICKS_PER_BURST = 2     # Minimum licks to count as a valid burst\n",
    "\n",
    "# Burst Alignment Mode for 'burst_t':\n",
    "# \"first\"  : Aligns to the very first lick\n",
    "# \"second\" : Aligns to the second lick\n",
    "BURST_EVENT_MODE = \"first\"\n",
    "\n",
    "def apply_window(t: np.ndarray, *series: np.ndarray, plot_window=None):\n",
    "    \"\"\"Restricts time and data arrays to a specific time window.\"\"\"\n",
    "    if plot_window is None:\n",
    "        mask = np.ones_like(t, dtype=bool)\n",
    "        return (t, *series, mask)\n",
    "\n",
    "    lo, hi = float(plot_window[0]), float(plot_window[1])\n",
    "    if lo > hi: lo, hi = hi, lo\n",
    "\n",
    "    mask = (t >= lo) & (t <= hi)\n",
    "    t_masked = t[mask]\n",
    "    series_masked = [s[mask] if s is not None else None for s in series]\n",
    "    return (t_masked, *series_masked, mask)\n",
    "\n",
    "def detect_lick_times(t: np.ndarray, dio: np.ndarray, threshold=0.5, min_event_gap_s=0.02):\n",
    "    \"\"\"Converts raw analog DIO signal into discrete timestamps with debouncing.\"\"\"\n",
    "    t = np.asarray(t, float)\n",
    "    x = np.asarray(dio, float)\n",
    "    if t.size == 0 or x.size == 0:\n",
    "        return np.array([], dtype=float)\n",
    "\n",
    "    binary = x >= float(threshold)\n",
    "    rising_indices = np.flatnonzero(np.diff(binary.astype(np.int8), prepend=binary[0].astype(np.int8)) == 1)\n",
    "    lick_t = t[rising_indices].astype(float)\n",
    "\n",
    "    if lick_t.size <= 1:\n",
    "        return lick_t\n",
    "\n",
    "    # Debounce\n",
    "    dt = np.diff(lick_t)\n",
    "    keep = np.ones_like(lick_t, dtype=bool)\n",
    "    keep[1:] = dt >= float(min_event_gap_s)\n",
    "\n",
    "    return lick_t[keep]\n",
    "\n",
    "def get_burst_threshold(lick_t: np.ndarray, bout_gap_s=\"auto\"):\n",
    "    \"\"\"Calculates the burst separation threshold using log-KMeans if 'auto'.\"\"\"\n",
    "    if lick_t.size < 2:\n",
    "        return 0.5 if bout_gap_s == \"auto\" else float(bout_gap_s)\n",
    "\n",
    "    if bout_gap_s != \"auto\":\n",
    "        return float(bout_gap_s)\n",
    "\n",
    "    iei = np.diff(lick_t)\n",
    "    valid_iei = iei[(iei > 0) & (iei < 60)]\n",
    "\n",
    "    if valid_iei.size <= 5:\n",
    "        return 0.5\n",
    "\n",
    "    log_iei = np.log10(valid_iei)\n",
    "    c1 = np.percentile(log_iei, 20)\n",
    "    c2 = np.percentile(log_iei, 80)\n",
    "\n",
    "    for _ in range(10):\n",
    "        d1 = np.abs(log_iei - c1)\n",
    "        d2 = np.abs(log_iei - c2)\n",
    "        mask1 = d1 < d2\n",
    "        if np.any(mask1): c1 = np.mean(log_iei[mask1])\n",
    "        if np.any(~mask1): c2 = np.mean(log_iei[~mask1])\n",
    "\n",
    "    split_log = (c1 + c2) / 2.0\n",
    "    calc_thresh = 10**split_log\n",
    "\n",
    "    return max(0.2, min(calc_thresh, 5.0))\n",
    "\n",
    "def burst_events_from_licks(lick_t: np.ndarray, bout_gap_s=\"auto\", mode=\"first\", min_licks=1):\n",
    "    \"\"\"\n",
    "    Groups licks into bursts.\n",
    "    Returns:\n",
    "      burst_times: Main alignment timestamp (based on 'mode')\n",
    "      burst_sizes: Count of licks in each burst\n",
    "      ibi: Inter-burst intervals\n",
    "      threshold: The gap threshold used\n",
    "      onset_second_lick: Specific timestamp of the 2nd lick for every burst (NaN if size < 2)\n",
    "      onset_first_lick: Specific timestamp of the 1st lick for every burst (NaN if size < 1)\n",
    "    \"\"\"\n",
    "    lick_t = np.asarray(lick_t, float)\n",
    "    if lick_t.size == 0:\n",
    "        return np.array([]), np.array([], dtype=int), np.array([]), 0.5, np.array([]), np.array([])\n",
    "\n",
    "    # 1. Threshold\n",
    "    threshold = get_burst_threshold(lick_t, bout_gap_s)\n",
    "\n",
    "    # 2. Grouping\n",
    "    iei = np.diff(lick_t)\n",
    "    new_burst = np.r_[True, iei > threshold]\n",
    "    burst_ids = np.cumsum(new_burst) - 1\n",
    "\n",
    "    unique_bursts = np.unique(burst_ids)\n",
    "\n",
    "    burst_times = []\n",
    "    burst_sizes = []\n",
    "    onset_second_lick = []  # New collection list\n",
    "    onset_first_lick = []   # New collection list for first lick\n",
    "\n",
    "    # 3. Filtering & Timestamping\n",
    "    for b in unique_bursts:\n",
    "        lt = lick_t[burst_ids == b]\n",
    "        n_licks = lt.size\n",
    "\n",
    "        if n_licks < min_licks:\n",
    "            continue\n",
    "\n",
    "        # Determine Main Alignment Timestamp\n",
    "        timestamp = np.nan\n",
    "        if mode == \"first\":\n",
    "            timestamp = lt[0]\n",
    "        elif mode == \"second\":\n",
    "            if n_licks >= 2:\n",
    "                timestamp = lt[1]\n",
    "            else:\n",
    "                continue\n",
    "        elif mode == \"center\":\n",
    "            timestamp = np.mean(lt)\n",
    "\n",
    "        # Determine \"Second Lick\" Timestamp (Always capture if available)\n",
    "        second_val = lt[1] if n_licks >= 2 else np.nan\n",
    "\n",
    "        # Determine \"First Lick\" Timestamp (Always capture if available)\n",
    "        first_val = lt[0] if n_licks >= 1 else np.nan\n",
    "\n",
    "        burst_times.append(timestamp)\n",
    "        burst_sizes.append(n_licks)\n",
    "        onset_second_lick.append(second_val)\n",
    "        onset_first_lick.append(first_val)\n",
    "\n",
    "    burst_times = np.array(burst_times, dtype=float)\n",
    "    burst_sizes = np.array(burst_sizes, dtype=int)\n",
    "    onset_second_lick = np.array(onset_second_lick, dtype=float)\n",
    "    onset_first_lick = np.array(onset_first_lick, dtype=float)\n",
    "\n",
    "    ibi = np.diff(burst_times) if burst_times.size >= 2 else np.array([])\n",
    "\n",
    "    return burst_times, burst_sizes, ibi, threshold, onset_second_lick, onset_first_lick\n",
    "\n",
    "# ---- Main Analysis Loop ----\n",
    "\n",
    "if 'results' not in locals():\n",
    "    results = {}\n",
    "    print(\"Warning: 'results' dictionary not found. Please load your data first.\")\n",
    "\n",
    "per_animal = {}\n",
    "\n",
    "for animal_id, rec in results.items():\n",
    "    t = rec[\"t\"]\n",
    "    dio = rec[\"dio2\"]\n",
    "\n",
    "    t_w, dio_w, _ = apply_window(t, dio, plot_window=PLOT_WINDOW)\n",
    "\n",
    "    lick_t = detect_lick_times(\n",
    "        t_w, dio_w, threshold=DIO_THRESHOLD, min_event_gap_s=MIN_EVENT_GAP_S\n",
    "    )\n",
    "\n",
    "    # Updated unpacking to include first lick onset\n",
    "    burst_t, burst_sizes, ibi, calc_thresh, lick_onset_2nd, lick_onset_1st = burst_events_from_licks(\n",
    "        lick_t, bout_gap_s=BOUT_GAP_S, mode=BURST_EVENT_MODE, min_licks=MIN_LICKS_PER_BURST\n",
    "    )\n",
    "\n",
    "    per_animal[animal_id] = {\n",
    "        \"n_bursts\": int(burst_t.size),\n",
    "        \"burst_t\": burst_t,            # Primary alignment timestamp\n",
    "        \"lick_onset\": lick_onset_2nd,  # Timestamp of the 2nd lick\n",
    "        \"first_lick_onset\": lick_onset_1st,  # Timestamp of the 1st lick\n",
    "        \"ibi\": ibi,\n",
    "        \"burst_sizes\": burst_sizes,\n",
    "        \"lick_t\": lick_t,\n",
    "        \"calc_thresh\": calc_thresh\n",
    "    }\n",
    "\n",
    "# ---- Visualization ----\n",
    "if len(per_animal) > 0:\n",
    "    animal_ids = sorted(per_animal.keys())\n",
    "\n",
    "    # --- Plot 1: Burst Counts ---\n",
    "    n_bursts = [per_animal[a][\"n_bursts\"] for a in animal_ids]\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.bar(animal_ids, n_bursts, color=\"#9b59b6\", alpha=0.9)\n",
    "    plt.ylabel(\"Number of Bursts\")\n",
    "    plt.title(f\"Burst Count (Mode: {BURST_EVENT_MODE}, Min Licks: {MIN_LICKS_PER_BURST})\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Plot 2: Mean IBI (Inter-Burst Interval) ---\n",
    "    mean_ibis = []\n",
    "    for a in animal_ids:\n",
    "        ibis = per_animal[a][\"ibi\"]\n",
    "        val = np.nanmean(ibis) if ibis.size > 0 else np.nan\n",
    "        mean_ibis.append(val)\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.bar(animal_ids, mean_ibis, color=\"#e67e22\", alpha=0.9)\n",
    "    plt.ylabel(\"Mean IBI (s)\")\n",
    "    plt.title(\"Mean Inter-Burst Interval\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Plot 3: Mean Licks Per Burst (NEW) ---\n",
    "    mean_licks = []\n",
    "    for a in animal_ids:\n",
    "        sizes = per_animal[a][\"burst_sizes\"]\n",
    "        val = np.mean(sizes) if sizes.size > 0 else 0\n",
    "        mean_licks.append(val)\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.bar(animal_ids, mean_licks, color=\"#2ecc71\", alpha=0.9) # Green\n",
    "    plt.ylabel(\"Avg Licks per Burst\")\n",
    "    plt.title(\"Mean Licks per Burst\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Plot 4: Raster with Burst Overlays ---\n",
    "    plt.figure(figsize=(12, max(4, len(animal_ids) * 0.8)))\n",
    "\n",
    "    for i, animal_id in enumerate(animal_ids):\n",
    "        data = per_animal[animal_id]\n",
    "        lick_t = data[\"lick_t\"]\n",
    "        thresh = data[\"calc_thresh\"]\n",
    "\n",
    "        if lick_t.size == 0:\n",
    "            continue\n",
    "\n",
    "        iei = np.diff(lick_t)\n",
    "        is_new_burst = np.r_[True, iei > thresh]\n",
    "        burst_ids = np.cumsum(is_new_burst) - 1\n",
    "\n",
    "        # Ticks\n",
    "        plt.plot(lick_t, np.full_like(lick_t, i), '|', color='black',\n",
    "                 markersize=8, markeredgewidth=1, alpha=0.3)\n",
    "\n",
    "        # Bars\n",
    "        unique_bursts = np.unique(burst_ids)\n",
    "        for bid in unique_bursts:\n",
    "            mask = burst_ids == bid\n",
    "            ts = lick_t[mask]\n",
    "\n",
    "            if len(ts) < MIN_LICKS_PER_BURST:\n",
    "                continue\n",
    "\n",
    "            t_start, t_end = ts[0], ts[-1]\n",
    "\n",
    "            if t_end > t_start:\n",
    "                plt.hlines(i, t_start, t_end, color='#8e44ad', linewidth=5, alpha=0.6)\n",
    "            else:\n",
    "                plt.plot(t_start, i, 'o', color='#8e44ad', markersize=4, alpha=0.6)\n",
    "\n",
    "        plt.text(lick_t.max(), i, f\" Thresh: {thresh:.2f}s\", va='center', fontsize=8, color='gray')\n",
    "\n",
    "    plt.yticks(range(len(animal_ids)), animal_ids)\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Animal ID\")\n",
    "    plt.title(f\"Lick Raster & Burst Intervals\")\n",
    "    plt.grid(True, axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data found in 'per_animal'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d6c194a738e639b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T09:44:52.990356Z",
     "start_time": "2026-01-05T09:44:52.982325Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Helper Function: Compute PSTH\n",
    "# =============================================================================\n",
    "def compute_psth_matrix(t, dff, events, window=(-3, 5), baseline_win=(-3, -1), smooth_sigma_s=0.1):\n",
    "    \"\"\"\n",
    "    Computes the trial-by-trial Z-scored PSTH matrix.\n",
    "    \"\"\"\n",
    "    # Estimate sampling rate\n",
    "    fs = 1.0 / np.nanmedian(np.diff(t))\n",
    "\n",
    "    # Convert time windows to indices\n",
    "    n_pre = int(abs(window[0]) * fs)\n",
    "    n_post = int(abs(window[1]) * fs)\n",
    "    epoch_len = n_pre + n_post\n",
    "\n",
    "    # Baseline indices relative to event onset (0)\n",
    "    base_start_idx = int(baseline_win[0] * fs)\n",
    "    base_end_idx = int(baseline_win[1] * fs)\n",
    "\n",
    "    # Initialize matrix\n",
    "    n_events = len(events)\n",
    "    psth_mat = np.full((n_events, epoch_len), np.nan)\n",
    "\n",
    "    valid_trials = 0\n",
    "\n",
    "    for i, event_t in enumerate(events):\n",
    "        if np.isnan(event_t):\n",
    "            continue\n",
    "\n",
    "        # Find index of event in time array\n",
    "        idx = np.searchsorted(t, event_t)\n",
    "\n",
    "        # Define indices for the epoch\n",
    "        idx_start = idx + int(window[0] * fs)\n",
    "        idx_end = idx_start + epoch_len\n",
    "\n",
    "        # Check bounds\n",
    "        if idx_start >= 0 and idx_end <= len(dff):\n",
    "            raw_epoch = dff[idx_start:idx_end]\n",
    "\n",
    "            # --- Z-Score Calculation (Baseline specific) ---\n",
    "            # Offset = (baseline_start - window_start)\n",
    "            b_s = int((baseline_win[0] - window[0]) * fs)\n",
    "            b_e = int((baseline_win[1] - window[0]) * fs)\n",
    "\n",
    "            # Safety clip\n",
    "            b_s = max(0, b_s)\n",
    "            b_e = min(epoch_len, b_e)\n",
    "\n",
    "            baseline_data = raw_epoch[b_s:b_e]\n",
    "\n",
    "            if len(baseline_data) > 0:\n",
    "                mu = np.mean(baseline_data)\n",
    "                sigma = np.std(baseline_data)\n",
    "\n",
    "                if sigma > 0:\n",
    "                    z_epoch = (raw_epoch - mu) / sigma\n",
    "                    psth_mat[i, :] = z_epoch\n",
    "                    valid_trials += 1\n",
    "\n",
    "    # Generate time vector for plotting\n",
    "    tvec = np.linspace(window[0], window[1], epoch_len)\n",
    "\n",
    "    # Optional Smoothing\n",
    "    if smooth_sigma_s > 0:\n",
    "        sigma_samples = smooth_sigma_s * fs\n",
    "        psth_mat = gaussian_filter1d(psth_mat, sigma=sigma_samples, axis=1)\n",
    "\n",
    "    return tvec, psth_mat\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79686d6cd3d28b9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T09:44:53.971374Z",
     "start_time": "2026-01-05T09:44:52.998487Z"
    }
   },
   "outputs": [],
   "source": [
    "# ---- NEW: Dictionary to store results for later usage ----\n",
    "psth_results = {}\n",
    "\n",
    "for animal_id, metrics in per_animal.items():\n",
    "    # 1. Get raw data from original results\n",
    "    if animal_id not in results:\n",
    "        continue\n",
    "\n",
    "    raw_data = results[animal_id]\n",
    "    t = raw_data[\"t\"]\n",
    "    dff = raw_data[\"dff\"]\n",
    "\n",
    "    # 2. Extract Events (2nd lick of burst)\n",
    "    events = metrics[\"lick_onset\"]\n",
    "    events = events[~np.isnan(events)] # Remove NaNs\n",
    "\n",
    "    print(f\"Animal {animal_id}: Found {len(events)} valid events (2nd lick of burst).\")\n",
    "\n",
    "    if len(events) < 5:\n",
    "        print(f\"  -> Skipping (Not enough trials)\")\n",
    "        continue\n",
    "\n",
    "    # 3. Compute PSTH\n",
    "    tvec, mat = compute_psth_matrix(\n",
    "        t, dff, events,\n",
    "        window=(-3, 5),          # 3s before to 5s after\n",
    "        baseline_win=(-3, 0),    # Baseline: -3s to 0s\n",
    "        smooth_sigma_s=0\n",
    "    )\n",
    "\n",
    "    # Remove rows that stayed NaN (boundary issues)\n",
    "    mat = mat[~np.isnan(mat).all(axis=1)]\n",
    "\n",
    "    if len(mat) == 0:\n",
    "        continue\n",
    "\n",
    "    # ---- SAVE TO DICTIONARY ----\n",
    "    psth_results[animal_id] = {\n",
    "        \"time_vector\": tvec,\n",
    "        \"psth_matrix\": mat,\n",
    "        \"mean_trace\": np.nanmean(mat, axis=0),\n",
    "        \"sem_trace\": np.nanstd(mat, axis=0) / np.sqrt(mat.shape[0]),\n",
    "        \"n_trials\": mat.shape[0]\n",
    "    }\n",
    "\n",
    "    # 4. Plot (Optional - can be commented out if you just want to save)\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 8), sharex=True,\n",
    "                                   gridspec_kw={'height_ratios': [2, 1]})\n",
    "\n",
    "    # Get first lick onsets for valid events\n",
    "    first_licks = metrics[\"first_lick_onset\"]\n",
    "    first_licks = first_licks[~np.isnan(metrics[\"lick_onset\"])]  # Align with filtered events\n",
    "    relative_first_licks = first_licks - events  # Relative time of first lick to second lick\n",
    "    \n",
    "    # Compute average relative time for first lick (for vertical lines)\n",
    "    mean_relative_first = np.nanmean(relative_first_licks)\n",
    "\n",
    "    # A. Heatmap\n",
    "    im = ax1.imshow(mat, aspect='auto', origin='lower',\n",
    "                    extent=[tvec[0], tvec[-1], 0, mat.shape[0]],\n",
    "                    cmap='viridis', vmin=-3, vmax=3)\n",
    "\n",
    "    ax1.set_ylabel(\"Trial #\")\n",
    "    ax1.set_title(f\"Animal {animal_id}: 2nd Lick-Aligned PSTH\")\n",
    "    plt.colorbar(im, ax=ax1, label=\"Z-score\")\n",
    "    ax1.axvline(0, color='r', linestyle='--', alpha=0.7, label=\"2nd Lick\")\n",
    "    ax1.axvline(mean_relative_first, color='b', linestyle='--', alpha=0.7, label=\"1st Lick\")\n",
    "\n",
    "    # B. Average Response\n",
    "    mean_trace = psth_results[animal_id][\"mean_trace\"]\n",
    "    sem_trace = psth_results[animal_id][\"sem_trace\"]\n",
    "\n",
    "    ax2.plot(tvec, mean_trace, color='k', lw=2, label=\"Mean Response\")\n",
    "    ax2.fill_between(tvec, mean_trace - sem_trace, mean_trace + sem_trace,\n",
    "                     color='k', alpha=0.2, label=\"SEM\")\n",
    "\n",
    "    ax2.axvline(0, color='r', linestyle='--', alpha=0.7, label=\"2nd Lick Onset\")\n",
    "    ax2.axvline(mean_relative_first, color='b', linestyle='--', alpha=0.7, label=\"1st Lick Onset\")\n",
    "    ax2.axhline(0, color='gray', linestyle=':', alpha=0.5)\n",
    "    ax2.set_xlabel(\"Time from Event (s)\")\n",
    "    ax2.set_ylabel(\"Z-score\")\n",
    "    ax2.set_xlim(tvec[0], tvec[-1])\n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nProcessing Complete.\")\n",
    "print(f\"PSTH data saved in 'psth_results' for {len(psth_results)} animals.\")\n",
    "# Usage example: psth_results['Animal_01']['psth_matrix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1922288fd57d80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T09:44:54.454369Z",
     "start_time": "2026-01-05T09:44:54.088952Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "# Settings\n",
    "AUC_WINDOW = (0, 4)  # Window for AUC calculation (seconds)\n",
    "common_fs = 1.0 / np.nanmedian(np.diff(psth_results[list(psth_results.keys())[0]][\"time_vector\"]))\n",
    "\n",
    "print(f\"Analyzing correlation between Lick Count and PSTH AUC ({AUC_WINDOW[0]}s to {AUC_WINDOW[1]}s)...\")\n",
    "\n",
    "# Store correlation results for summary\n",
    "correlation_summary = {}\n",
    "\n",
    "# Setup Plotting Grid (dynamic based on n_animals)\n",
    "n_animals = len(psth_results)\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(n_animals / n_cols))\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4 * n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, animal_id in enumerate(sorted(psth_results.keys())):\n",
    "    ax = axes[i]\n",
    "\n",
    "    # 1. Retrieve Data\n",
    "    psth_data = psth_results[animal_id]\n",
    "    mat = psth_data[\"psth_matrix\"]\n",
    "    tvec = psth_data[\"time_vector\"]\n",
    "\n",
    "    # Retrieve trial-by-trial lick counts (burst sizes) from the metrics dict\n",
    "    # Note: We must ensure alignment. The PSTH matrix rows correspond to the valid events in 'lick_onset'.\n",
    "    # In the previous script, we filtered 'lick_onset' for NaNs. We must apply the same filter to 'burst_sizes'.\n",
    "    raw_burst_sizes = per_animal[animal_id][\"burst_sizes\"]\n",
    "    raw_lick_onsets = per_animal[animal_id][\"lick_onset\"]\n",
    "\n",
    "    # Filter: Keep only trials that were valid for PSTH (i.e., had a valid 2nd lick)\n",
    "    valid_mask = ~np.isnan(raw_lick_onsets)\n",
    "    trial_lick_counts = raw_burst_sizes[valid_mask]\n",
    "\n",
    "    # Double Check Alignment: Matrix rows vs Lick Count length\n",
    "    # If PSTH matrix dropped rows due to boundary issues (edges of recording), we must truncate the lick counts too.\n",
    "    # The previous script did: mat = mat[~np.isnan(mat).all(axis=1)]\n",
    "    # We need to assume the 'mat' in psth_results corresponds to the *start* of the valid list.\n",
    "    # If rows were dropped from the *end* or *middle* blindly, alignment is lost.\n",
    "    # Ideally, we would have stored the 'valid_indices' in psth_results.\n",
    "    # FORCE ALIGNMENT: Truncate to the shorter length (assuming chronological drop from ends if any)\n",
    "    n_trials = min(len(trial_lick_counts), mat.shape[0])\n",
    "    trial_lick_counts = trial_lick_counts[:n_trials]\n",
    "    mat = mat[:n_trials, :]\n",
    "\n",
    "    # 2. Calculate AUC for each trial\n",
    "    # Find indices for the AUC window\n",
    "    idx_start = np.searchsorted(tvec, AUC_WINDOW[0])\n",
    "    idx_end = np.searchsorted(tvec, AUC_WINDOW[1])\n",
    "\n",
    "    # Extract window and compute AUC (Trapezoidal rule)\n",
    "    # axis=1 integrates along time\n",
    "    auc_values = np.trapz(mat[:, idx_start:idx_end], dx=1/common_fs, axis=1)\n",
    "\n",
    "    # 3. Correlation\n",
    "    r_val, p_val = pearsonr(trial_lick_counts, auc_values)\n",
    "    correlation_summary[animal_id] = (r_val, p_val)\n",
    "\n",
    "    # 4. Plot\n",
    "    # Scatter points\n",
    "    ax.scatter(trial_lick_counts, auc_values, alpha=0.6, color='tab:blue', edgecolor='k')\n",
    "\n",
    "    # Linear Regression Line\n",
    "    m, b = np.polyfit(trial_lick_counts, auc_values, 1)\n",
    "    x_fit = np.array([trial_lick_counts.min(), trial_lick_counts.max()])\n",
    "    ax.plot(x_fit, m*x_fit + b, color='tab:red', linestyle='--', lw=2, label=f\"Fit\")\n",
    "\n",
    "    # Styling\n",
    "    ax.set_title(f\"{animal_id}\\nR={r_val:.2f}, p={p_val:.3f}\")\n",
    "    ax.set_xlabel(\"Licks per Burst\")\n",
    "    ax.set_ylabel(f\"AUC ({AUC_WINDOW[0]}-{AUC_WINDOW[1]}s)\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Hide empty subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Summary Plot of Correlations ---\n",
    "plt.figure(figsize=(8, 4))\n",
    "r_values = [v[0] for v in correlation_summary.values()]\n",
    "ids = list(correlation_summary.keys())\n",
    "colors = ['tab:red' if v[1] < 0.05 else 'gray' for v in correlation_summary.values()]\n",
    "\n",
    "plt.bar(ids, r_values, color=colors)\n",
    "plt.axhline(0, color='k', linewidth=1)\n",
    "plt.ylabel(\"Pearson Correlation (r)\")\n",
    "plt.title(\"Correlation: Lick Count vs. Dopamine AUC\\n(Red = Significant p < 0.05)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28100b20f57fe5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T09:44:54.560675Z",
     "start_time": "2026-01-05T09:44:54.460463Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr, linregress\n",
    "\n",
    "# =============================================================================\n",
    "# Global Correlation Analysis: Pooled Data (All Animals)\n",
    "# =============================================================================\n",
    "\n",
    "# Settings\n",
    "AUC_WINDOW = (-5, 5)  # Window for AUC calculation (seconds)\n",
    "\n",
    "# Containers for pooled data\n",
    "all_lick_counts = []\n",
    "all_auc_values = []\n",
    "animal_colors = [] # Optional: to color-code points by animal\n",
    "\n",
    "# Setup colormap for distinguishing animals in the scatter\n",
    "cmap = plt.get_cmap('tab10')\n",
    "animal_ids = sorted(psth_results.keys())\n",
    "\n",
    "print(f\"Aggregating data for Global Correlation ({AUC_WINDOW[0]}s to {AUC_WINDOW[1]}s)...\")\n",
    "\n",
    "for i, animal_id in enumerate(animal_ids):\n",
    "    # 1. Retrieve Data\n",
    "    psth_data = psth_results[animal_id]\n",
    "    mat = psth_data[\"psth_matrix\"]\n",
    "    tvec = psth_data[\"time_vector\"]\n",
    "\n",
    "    # Estimate sampling rate from time vector\n",
    "    fs = 1.0 / np.nanmedian(np.diff(tvec))\n",
    "\n",
    "    # 2. Retrieve & Filter Behavioral Data\n",
    "    # Get raw arrays\n",
    "    raw_burst_sizes = per_animal[animal_id][\"burst_sizes\"]\n",
    "    raw_lick_onsets = per_animal[animal_id][\"lick_onset\"]\n",
    "\n",
    "    # Apply the same filter used to generate the PSTH events:\n",
    "    # Keep only bursts that had a valid \"2nd lick\" timestamp (i.e., not NaN)\n",
    "    valid_behavior_mask = ~np.isnan(raw_lick_onsets)\n",
    "    trial_lick_counts = raw_burst_sizes[valid_behavior_mask]\n",
    "\n",
    "    # 3. Align Behavioral Data with PSTH Matrix\n",
    "    # The PSTH matrix might be shorter if trials at the very end of the recording\n",
    "    # were dropped due to window boundaries.\n",
    "    n_psth_trials = mat.shape[0]\n",
    "    n_behavior_trials = len(trial_lick_counts)\n",
    "\n",
    "    # We assume chronological order, so we truncate the longer array to match the shorter one.\n",
    "    n_valid = min(n_psth_trials, n_behavior_trials)\n",
    "\n",
    "    # Truncate\n",
    "    trial_lick_counts = trial_lick_counts[:n_valid]\n",
    "    mat_aligned = mat[:n_valid, :]\n",
    "\n",
    "    # 4. Calculate AUC for valid trials\n",
    "    # Find indices for the integration window\n",
    "    idx_start = np.searchsorted(tvec, AUC_WINDOW[0])\n",
    "    idx_end = np.searchsorted(tvec, AUC_WINDOW[1])\n",
    "\n",
    "    # Compute AUC using Trapezoidal rule along axis 1 (time)\n",
    "    auc_values = np.trapz(mat_aligned[:, idx_start:idx_end], dx=1/fs, axis=1)\n",
    "\n",
    "    # 5. Append to Global Lists\n",
    "    all_lick_counts.extend(trial_lick_counts)\n",
    "    all_auc_values.extend(auc_values)\n",
    "\n",
    "    # Store color index for this batch of points\n",
    "    animal_colors.extend([i] * n_valid)\n",
    "\n",
    "# Convert to numpy arrays for analysis\n",
    "all_lick_counts = np.array(all_lick_counts)\n",
    "all_auc_values = np.array(all_auc_values)\n",
    "animal_colors = np.array(animal_colors)\n",
    "\n",
    "print(f\"Total Trials Analyzed: {len(all_lick_counts)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# Statistical Analysis & Plotting\n",
    "# =============================================================================\n",
    "\n",
    "if len(all_lick_counts) > 5:\n",
    "    # 1. Pearson Correlation\n",
    "    r_val, p_val = pearsonr(all_lick_counts, all_auc_values)\n",
    "\n",
    "    # 2. Linear Regression (for the fit line)\n",
    "    slope, intercept, _, _, _ = linregress(all_lick_counts, all_auc_values)\n",
    "\n",
    "    # 3. Plotting\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Scatter Plot (Color-coded by animal)\n",
    "    scatter = plt.scatter(all_lick_counts, all_auc_values,\n",
    "                          c=animal_colors, cmap='tab10',\n",
    "                          alpha=0.6, s=25, edgecolor='w', linewidth=0.5)\n",
    "\n",
    "    # Regression Line\n",
    "    x_range = np.linspace(all_lick_counts.min(), all_lick_counts.max(), 100)\n",
    "    y_fit = slope * x_range + intercept\n",
    "\n",
    "    plt.plot(x_range, y_fit, color='black', linestyle='--', linewidth=2.5,\n",
    "             label=f\"Linear Fit\\ny = {slope:.2f}x + {intercept:.2f}\")\n",
    "\n",
    "    # Aesthetics\n",
    "    plt.title(f\"Global Correlation: Lick Count vs. Dopamine AUC\\n(n={len(animal_ids)} Animals, {len(all_lick_counts)} Trials)\", fontsize=14)\n",
    "    plt.xlabel(\"Number of Licks per Burst\", fontsize=12)\n",
    "    plt.ylabel(f\"PSTH AUC (Z-score * sec)\\nWindow: {AUC_WINDOW[0]}s to {AUC_WINDOW[1]}s\", fontsize=12)\n",
    "\n",
    "    # Legend for Statistics\n",
    "    stats_text = f\"Pearson r = {r_val:.3f}\\np-value = {p_val:.2e}\"\n",
    "    plt.text(0.05, 0.95, stats_text, transform=plt.gca().transAxes,\n",
    "             fontsize=12, verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))\n",
    "\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True, linestyle=':', alpha=0.6)\n",
    "\n",
    "    # Optional: Create a custom legend for animals\n",
    "    # handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=cmap(i), label=aid) for i, aid in enumerate(animal_ids)]\n",
    "    # plt.legend(handles=handles, title=\"Animal ID\", loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"Insufficient data for correlation analysis.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afc066e4a4395a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T09:44:54.707954Z",
     "start_time": "2026-01-05T09:44:54.568819Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr, linregress\n",
    "\n",
    "# =============================================================================\n",
    "# Animal-Level Correlation: Mean Licks vs. Mean AUC\n",
    "# =============================================================================\n",
    "\n",
    "# Settings\n",
    "AUC_WINDOW = (-1, 4)  # Window for AUC calculation (seconds)\n",
    "\n",
    "# Containers for animal-level means\n",
    "mean_licks_per_animal = []\n",
    "mean_auc_per_animal = []\n",
    "animal_ids_list = []\n",
    "\n",
    "print(f\"Computing per-animal averages for Correlation ({AUC_WINDOW[0]}s to {AUC_WINDOW[1]}s)...\")\n",
    "\n",
    "for animal_id in sorted(psth_results.keys()):\n",
    "    # 1. Retrieve Data\n",
    "    psth_data = psth_results[animal_id]\n",
    "    mat = psth_data[\"psth_matrix\"]\n",
    "    tvec = psth_data[\"time_vector\"]\n",
    "\n",
    "    # 2. Compute Mean AUC for this animal\n",
    "    # Method: Calculate AUC for the *average trace* (more robust to noise)\n",
    "    # Alternatively, you could calc AUC per trial then mean, but for Z-scores, linear operations are commutative.\n",
    "\n",
    "    # Calculate Mean Trace first\n",
    "    mean_trace = np.nanmean(mat, axis=0)\n",
    "\n",
    "    # Estimate sampling rate\n",
    "    fs = 1.0 / np.nanmedian(np.diff(tvec))\n",
    "\n",
    "    # Find indices for integration\n",
    "    idx_start = np.searchsorted(tvec, AUC_WINDOW[0])\n",
    "    idx_end = np.searchsorted(tvec, AUC_WINDOW[1])\n",
    "\n",
    "    # Compute AUC of the mean trace\n",
    "    animal_auc = np.trapz(mean_trace[idx_start:idx_end], dx=1/fs)\n",
    "\n",
    "    # 3. Compute Mean Licks for this animal\n",
    "    # Retrieve raw burst sizes\n",
    "    raw_burst_sizes = per_animal[animal_id][\"burst_sizes\"]\n",
    "\n",
    "    # Filter for valid bursts only (same filter as PSTH events)\n",
    "    raw_lick_onsets = per_animal[animal_id][\"lick_onset\"]\n",
    "    valid_mask = ~np.isnan(raw_lick_onsets)\n",
    "    valid_burst_sizes = raw_burst_sizes[valid_mask]\n",
    "\n",
    "    # Calculate mean lick count\n",
    "    if len(valid_burst_sizes) > 0:\n",
    "        animal_mean_licks = np.mean(valid_burst_sizes)\n",
    "    else:\n",
    "        print(f\"Warning: No valid bursts for {animal_id}\")\n",
    "        continue\n",
    "\n",
    "    # 4. Store Data\n",
    "    mean_licks_per_animal.append(animal_mean_licks)\n",
    "    mean_auc_per_animal.append(animal_auc)\n",
    "    animal_ids_list.append(animal_id)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "x_vals = np.array(mean_licks_per_animal)\n",
    "y_vals = np.array(mean_auc_per_animal)\n",
    "\n",
    "# =============================================================================\n",
    "# Plotting & Statistics\n",
    "# =============================================================================\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "\n",
    "if len(x_vals) > 1:\n",
    "    # 1. Linear Regression\n",
    "    slope, intercept, r_val, p_val, std_err = linregress(x_vals, y_vals)\n",
    "\n",
    "    # 2. Plot Points\n",
    "    # Use a distinct color/marker for each animal\n",
    "    for i, aid in enumerate(animal_ids_list):\n",
    "        plt.scatter(x_vals[i], y_vals[i], s=150, zorder=3, label=aid, edgecolor='k')\n",
    "        # Annotate ID next to dot\n",
    "        plt.text(x_vals[i], y_vals[i], f\"  {aid}\", verticalalignment='center', fontsize=9)\n",
    "\n",
    "    # 3. Plot Fit Line\n",
    "    x_range = np.array([x_vals.min() * 0.9, x_vals.max() * 1.1])\n",
    "    plt.plot(x_range, slope * x_range + intercept, 'k--', alpha=0.5, zorder=2,\n",
    "             label=f\"Fit (r={r_val:.2f})\")\n",
    "\n",
    "    # 4. Stats Text\n",
    "    stats_msg = f\"Pearson r = {r_val:.3f}\\np = {p_val:.3f}\"\n",
    "    plt.text(0.05, 0.95, stats_msg, transform=plt.gca().transAxes,\n",
    "             fontsize=12, verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))\n",
    "\n",
    "else:\n",
    "    plt.scatter(x_vals, y_vals, s=100)\n",
    "    print(\"Not enough points for regression.\")\n",
    "\n",
    "# Aesthetics\n",
    "plt.title(\"Animal-Level Correlation\\n(Mean Licks vs. Mean AUC)\", fontsize=14)\n",
    "plt.xlabel(\"Average Licks per Burst\", fontsize=12)\n",
    "plt.ylabel(f\"Average PSTH AUC (Z-score)\\nWindow: {AUC_WINDOW[0]}s to {AUC_WINDOW[1]}s\", fontsize=12)\n",
    "plt.grid(True, linestyle=':', alpha=0.6)\n",
    "plt.legend(title=\"Animal ID\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print Data for verification\n",
    "print(\"\\n--- Summary Data ---\")\n",
    "for aid, licks, auc in zip(animal_ids_list, x_vals, y_vals):\n",
    "    print(f\"{aid}: {licks:.2f} licks/burst | {auc:.2f} AUC\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa18e8ea2e69ffa2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T09:44:54.942957Z",
     "start_time": "2026-01-05T09:44:54.713286Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =============================================================================\n",
    "# Aggregation and Plotting: Combined Heatmap and Individual Traces with Lick Alignments\n",
    "# =============================================================================\n",
    "\n",
    "# Container for animal averages\n",
    "animal_mean_traces = []\n",
    "animal_ids_list = []\n",
    "common_tvec = None\n",
    "\n",
    "print(\"Aggregating pre-computed per-animal averages...\")\n",
    "\n",
    "# Ensure psth_results exists\n",
    "if 'psth_results' not in locals():\n",
    "    print(\"Error: 'psth_results' dictionary not found. Please run the PSTH computation script first.\")\n",
    "else:\n",
    "    # Collect mean relative first lick times for each animal\n",
    "    animal_mean_relative_first = {}\n",
    "    \n",
    "    for animal_id, data in psth_results.items():\n",
    "        # 1. Retrieve Pre-computed Mean Trace\n",
    "        mean_trace = data.get(\"mean_trace\")\n",
    "        tvec = data.get(\"time_vector\")\n",
    "\n",
    "        if mean_trace is None or tvec is None:\n",
    "            print(f\"  Warning: Missing data for {animal_id}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # 2. Check for length consistency\n",
    "        if common_tvec is None:\n",
    "            common_tvec = tvec\n",
    "            expected_length = len(tvec)\n",
    "\n",
    "        if len(mean_trace) != expected_length:\n",
    "            print(f\"  Warning: Length mismatch for {animal_id} ({len(mean_trace)} vs {expected_length}). Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # 3. Compute mean relative first lick time for this animal\n",
    "        metrics = per_animal[animal_id]\n",
    "        events = metrics[\"lick_onset\"]\n",
    "        events = events[~np.isnan(events)]\n",
    "        first_licks = metrics[\"first_lick_onset\"]\n",
    "        first_licks = first_licks[~np.isnan(metrics[\"lick_onset\"])]\n",
    "        relative_first_licks = first_licks - events\n",
    "        mean_relative_first = np.nanmean(relative_first_licks)\n",
    "        animal_mean_relative_first[animal_id] = mean_relative_first\n",
    "\n",
    "        # 4. Store\n",
    "        animal_mean_traces.append(mean_trace)\n",
    "        animal_ids_list.append(animal_id)\n",
    "\n",
    "    # --- Plotting ---\n",
    "\n",
    "    if len(animal_mean_traces) > 0:\n",
    "        # Stack into matrix: (n_animals x n_timepoints)\n",
    "        grand_matrix = np.vstack(animal_mean_traces)\n",
    "        n_animals = grand_matrix.shape[0]\n",
    "\n",
    "        # Compute Grand Average and SEM across animals\n",
    "        grand_mean = np.nanmean(grand_matrix, axis=0)\n",
    "        grand_sem = np.nanstd(grand_matrix, axis=0) / np.sqrt(n_animals)\n",
    "\n",
    "        print(f\"Averaging across {n_animals} animals.\")\n",
    "\n",
    "        # Create figure with 2 subplots\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True, gridspec_kw={'height_ratios': [2, 1]})\n",
    "\n",
    "        # --- Subplot 1: Combined Heatmap (Each row is an animal) ---\n",
    "        im = ax1.imshow(grand_matrix, aspect='auto', origin='lower',\n",
    "                        extent=[common_tvec[0], common_tvec[-1], -0.5, n_animals - 0.5],\n",
    "                        cmap='viridis', vmin=-2, vmax=2)\n",
    "\n",
    "        ax1.set_yticks(range(n_animals))\n",
    "        ax1.set_yticklabels(animal_ids_list)\n",
    "        ax1.set_ylabel(\"Animal ID\")\n",
    "        ax1.set_title(\"Combined Mean Response per Animal (Heatmap)\")\n",
    "        \n",
    "        # Add vertical lines for licks (use average positions across animals for simplicity)\n",
    "        overall_mean_first = np.nanmean(list(animal_mean_relative_first.values()))\n",
    "        ax1.axvline(overall_mean_first, color='b', linestyle='--', alpha=0.8, linewidth=2, label=\"1st Lick\")\n",
    "        ax1.axvline(0, color='r', linestyle='--', alpha=0.8, linewidth=2, label=\"2nd Lick\")\n",
    "        ax1.legend(loc='upper right')\n",
    "        \n",
    "        plt.colorbar(im, ax=ax1, label=\"Z-score\")\n",
    "\n",
    "        # --- Subplot 2: Individual Z-score Traces and Grand Average ---\n",
    "        # Plot individual animal traces in distinct colors\n",
    "        colors = plt.cm.tab10(np.linspace(0, 1, n_animals))  # Use tab10 colormap for distinct colors\n",
    "        for i, (animal_id, trace) in enumerate(zip(animal_ids_list, grand_matrix)):\n",
    "            ax2.plot(common_tvec, trace, color=colors[i], lw=1.5, alpha=0.7, label=f\"{animal_id}\")\n",
    "\n",
    "        # Plot Grand Mean + SEM\n",
    "        ax2.plot(common_tvec, grand_mean, color='k', lw=3, label=f\"Grand Mean (n={n_animals})\")\n",
    "        ax2.fill_between(common_tvec, grand_mean - grand_sem, grand_mean + grand_sem,\n",
    "                        color='k', alpha=0.2, label=\"SEM\")\n",
    "\n",
    "        # Add vertical lines for licks\n",
    "        ax2.axvline(overall_mean_first, color='b', linestyle='--', alpha=0.8, linewidth=2, label=\"1st Lick Onset\")\n",
    "        ax2.axvline(0, color='r', linestyle='--', alpha=0.8, linewidth=2, label=\"2nd Lick Onset\")\n",
    "        ax2.axhline(0, color='gray', linestyle=':', alpha=0.5)\n",
    "        ax2.set_xlabel(\"Time from 2nd Lick Onset (s)\")\n",
    "        ax2.set_ylabel(\"Z-score\")\n",
    "        ax2.set_title(\"Individual Animal Traces and Grand Average\")\n",
    "        ax2.set_xlim(common_tvec[0], common_tvec[-1])\n",
    "        ax2.legend(loc='upper right', fontsize=8)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No valid data to average.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a8e369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "def compute_psth_matrix(t, y, event_times, window=(-3, 5), baseline_win=(-3, -1), resample_hz=30, smooth_sigma_s=0.1):\n",
    "    t = np.asarray(t, float)\n",
    "    y = np.asarray(y, float)\n",
    "    ev = np.asarray(event_times, float)\n",
    "    ev = ev[np.isfinite(ev)]\n",
    "    if ev.size == 0:\n",
    "        return np.array([], float), np.zeros((0, 0), float)\n",
    "    \n",
    "    dt = 1.0 / float(resample_hz)\n",
    "    tvec = np.arange(window[0], window[1] + 0.5 * dt, dt)\n",
    "    \n",
    "    mat = np.full((ev.size, tvec.size), np.nan, float)\n",
    "    \n",
    "    for i, et in enumerate(ev):\n",
    "        idx = np.searchsorted(t, et)\n",
    "        idx_start = idx + int(window[0] * 30)  # assuming fs=30\n",
    "        idx_end = idx_start + len(tvec)\n",
    "        if idx_start >= 0 and idx_end <= len(y):\n",
    "            raw_epoch = y[idx_start:idx_end]\n",
    "            b_s = int((baseline_win[0] - window[0]) * 30)\n",
    "            b_e = int((baseline_win[1] - window[0]) * 30)\n",
    "            b_s = max(0, b_s)\n",
    "            b_e = min(len(raw_epoch), b_e)\n",
    "            baseline_data = raw_epoch[b_s:b_e]\n",
    "            if len(baseline_data) > 0:\n",
    "                mu = np.mean(baseline_data)\n",
    "                sigma = np.std(baseline_data)\n",
    "                if sigma > 0:\n",
    "                    z_epoch = (raw_epoch - mu) / sigma\n",
    "                    mat[i, :] = z_epoch\n",
    "    \n",
    "    if smooth_sigma_s and smooth_sigma_s > 0:\n",
    "        sigma = smooth_sigma_s * resample_hz\n",
    "        mat = gaussian_filter1d(mat, sigma=sigma, axis=1, mode='nearest')\n",
    "    \n",
    "    return tvec, mat\n",
    "\n",
    "# Create licking bouts\n",
    "bout_onsets = {}\n",
    "bout_ends = {}\n",
    "\n",
    "for animal_id in per_animal:\n",
    "    lick_t = per_animal[animal_id]['lick_t']\n",
    "    if len(lick_t) < 2:\n",
    "        continue\n",
    "    iei = np.diff(lick_t)\n",
    "    thresh = per_animal[animal_id]['calc_thresh']\n",
    "    new_burst = np.r_[True, iei > thresh]\n",
    "    burst_ids = np.cumsum(new_burst) - 1\n",
    "    unique_bursts = np.unique(burst_ids)\n",
    "    onsets = []\n",
    "    ends = []\n",
    "    for b in unique_bursts:\n",
    "        lt = lick_t[burst_ids == b]\n",
    "        if len(lt) >= 2:\n",
    "            duration = lt[-1] - lt[0]\n",
    "            if duration > 1:\n",
    "                onsets.append(lt[0])\n",
    "                ends.append(lt[-1])\n",
    "    bout_onsets[animal_id] = np.array(onsets)\n",
    "    bout_ends[animal_id] = np.array(ends)\n",
    "\n",
    "# Compute PSTH for onsets\n",
    "onset_matrices = {}\n",
    "for animal_id in bout_onsets:\n",
    "    if len(bout_onsets[animal_id]) == 0:\n",
    "        continue\n",
    "    raw_data = results[animal_id]\n",
    "    t = raw_data['t']\n",
    "    dff = raw_data['dff']\n",
    "    events = bout_onsets[animal_id]\n",
    "    tvec, mat = compute_psth_matrix(t, dff, events, window=(-2, 1), baseline_win=(-2, -1), smooth_sigma_s=0)\n",
    "    onset_matrices[animal_id] = (tvec, mat)\n",
    "\n",
    "# Compute PSTH for ends\n",
    "end_matrices = {}\n",
    "for animal_id in bout_ends:\n",
    "    if len(bout_ends[animal_id]) == 0:\n",
    "        continue\n",
    "    raw_data = results[animal_id]\n",
    "    t = raw_data['t']\n",
    "    dff = raw_data['dff']\n",
    "    events = bout_ends[animal_id]\n",
    "    tvec, mat = compute_psth_matrix(t, dff, events, window=(-1, 4), baseline_win=(-1, 0), smooth_sigma_s=0)\n",
    "    end_matrices[animal_id] = (tvec, mat)\n",
    "\n",
    "# Plot heatmaps\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Onset heatmap\n",
    "all_mats_onset = []\n",
    "y_ticks_onset = []\n",
    "y_labels_onset = []\n",
    "cum = 0\n",
    "for aid in sorted(onset_matrices.keys()):\n",
    "    tvec, mat = onset_matrices[aid]\n",
    "    all_mats_onset.append(mat)\n",
    "    y_ticks_onset.append(cum + mat.shape[0] / 2)\n",
    "    y_labels_onset.append(aid)\n",
    "    cum += mat.shape[0]\n",
    "if all_mats_onset:\n",
    "    grand_mat_onset = np.vstack(all_mats_onset)\n",
    "    im1 = ax1.imshow(grand_mat_onset, aspect='auto', origin='lower', extent=[tvec[0], tvec[-1], 0, grand_mat_onset.shape[0]], cmap='viridis', vmin=-3, vmax=3)\n",
    "    ax1.set_title('Onset of Licking Bouts (-2 to +1 s)')\n",
    "    ax1.set_xlabel('Time (s)')\n",
    "    ax1.set_ylabel('Bouts')\n",
    "    ax1.set_yticks(y_ticks_onset)\n",
    "    ax1.set_yticklabels(y_labels_onset)\n",
    "    plt.colorbar(im1, ax=ax1, label='Z-score')\n",
    "\n",
    "# End heatmap\n",
    "all_mats_end = []\n",
    "y_ticks_end = []\n",
    "y_labels_end = []\n",
    "cum = 0\n",
    "for aid in sorted(end_matrices.keys()):\n",
    "    tvec, mat = end_matrices[aid]\n",
    "    all_mats_end.append(mat)\n",
    "    y_ticks_end.append(cum + mat.shape[0] / 2)\n",
    "    y_labels_end.append(aid)\n",
    "    cum += mat.shape[0]\n",
    "if all_mats_end:\n",
    "    grand_mat_end = np.vstack(all_mats_end)\n",
    "    im2 = ax2.imshow(grand_mat_end, aspect='auto', origin='lower', extent=[tvec[0], tvec[-1], 0, grand_mat_end.shape[0]], cmap='viridis', vmin=-3, vmax=3)\n",
    "    ax2.set_title('End of Licking Bouts (-1 to +4 s)')\n",
    "    ax2.set_xlabel('Time (s)')\n",
    "    ax2.set_ylabel('Bouts')\n",
    "    ax2.set_yticks(y_ticks_end)\n",
    "    ax2.set_yticklabels(y_labels_end)\n",
    "    plt.colorbar(im2, ax=ax2, label='Z-score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot PSTH averages with SEM\n",
    "if onset_matrices:\n",
    "    tvec_onset = onset_matrices[sorted(onset_matrices.keys())[0]][0]\n",
    "    mean_onset = np.nanmean(grand_mat_onset, axis=0)\n",
    "    sem_onset = np.nanstd(grand_mat_onset, axis=0) / np.sqrt(grand_mat_onset.shape[0])\n",
    "    \n",
    "if end_matrices:\n",
    "    tvec_end = end_matrices[sorted(end_matrices.keys())[0]][0]\n",
    "    mean_end = np.nanmean(grand_mat_end, axis=0)\n",
    "    sem_end = np.nanstd(grand_mat_end, axis=0) / np.sqrt(grand_mat_end.shape[0])\n",
    "\n",
    "fig2, (ax3, ax4) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "if onset_matrices:\n",
    "    ax3.plot(tvec_onset, mean_onset, 'k', lw=2)\n",
    "    ax3.fill_between(tvec_onset, mean_onset - sem_onset, mean_onset + sem_onset, alpha=0.3, color='k')\n",
    "    ax3.set_title('Average PSTH for Onset')\n",
    "    ax3.set_xlabel('Time (s)')\n",
    "    ax3.set_ylabel('Z-score')\n",
    "    ax3.axvline(0, color='r', linestyle='--')\n",
    "\n",
    "if end_matrices:\n",
    "    ax4.plot(tvec_end, mean_end, 'k', lw=2)\n",
    "    ax4.fill_between(tvec_end, mean_end - sem_end, mean_end + sem_end, alpha=0.3, color='k')\n",
    "    ax4.set_title('Average PSTH for End')\n",
    "    ax4.set_xlabel('Time (s)')\n",
    "    ax4.set_ylabel('Z-score')\n",
    "    ax4.axvline(0, color='r', linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show traces with bouts for each mouse\n",
    "for animal_id in sorted(results.keys()):\n",
    "    raw_data = results[animal_id]\n",
    "    t = raw_data['t']\n",
    "    dff = raw_data['zscore']\n",
    "    dio = raw_data['dio2']\n",
    "    onsets = bout_onsets.get(animal_id, np.array([]))\n",
    "    ends = bout_ends.get(animal_id, np.array([]))\n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize=(12, 4))\n",
    "    ax1.plot(t, dff, lw=1, color='blue')\n",
    "    ax1.set_xlabel('Time (s)')\n",
    "    ax1.set_ylabel('Z-score', color='blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='blue')\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(t, dio, color='orange', alpha=0.5)\n",
    "    ax2.set_ylabel('DIO', color='orange')\n",
    "    ax2.tick_params(axis='y', labelcolor='orange')\n",
    "    ax2.set_ylim(-0.1, 1.5)\n",
    "    \n",
    "    for start, end in zip(onsets, ends):\n",
    "        ax1.axvspan(start, end, alpha=0.3, color='red')\n",
    "    \n",
    "    plt.title(f'{animal_id}: Z-score Trace with Licking Bouts (red shaded)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
